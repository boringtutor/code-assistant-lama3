## Local Model Hosting:

We need a way to efficiently run the Lama 3 model on the user's machine.
Options:

# llama.cpp: This library provides a fast and optimized way to run large language models on consumer hardware.

# Custom Solution: We could potentially build our own lightweight inference engine if needed.

## VS Code Extension:

## SIMPLE STEPS :

# 1. RUN THE MODEL.CPP LOCALLY AND TEST THE INFERENCE

# 2. BUILD THE VS CODE EXTENSION

# 3. BULD THE GRPC CONNECTION TO THE MODEL.CPP

# 4. TEST THE CONNECTION BETWEEN THE VS CODE EXTENSION AND THE MODEL.CPP
